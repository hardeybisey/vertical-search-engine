{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Crawler for publications from the school of economics, finance and accounting of coventry University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "def mycrawler(maxcount):\n",
    "    crawled_data = []\n",
    "    count = 0\n",
    "    while (count < maxcount):\n",
    "        url = \"https://pureportal.coventry.ac.uk/en/organisations/school-of-economics-finance-and-accounting/publications/?page=\" + str(count)\n",
    "        print(\"fetching: \", url)\n",
    "        count +=1\n",
    "        html = requests.get(url)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "        publications = soup.findAll(\"h3\",{\"class\":\"title\"})\n",
    "        for p in publications:\n",
    "            publication_details = {}\n",
    "            publication_title = p.get_text()\n",
    "            publication_link = p.a.get(\"href\")\n",
    "            new_url = publication_link\n",
    "            html = requests.get(new_url)\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            time.sleep(1)\n",
    "            authors = soup.find(\"p\",{\"class\":\"relations persons\"}).get_text()  \n",
    "            get_abstract = soup.find(\"div\",{\"class\":\"textblock\"})\n",
    "            date = soup.find(\"span\", {\"class\":\"date\"}).get_text() \n",
    "            author_link = soup.findAll(\"a\",{\"class\":\"link person\"})            \n",
    "            author_links = []  \n",
    "            for link in author_link:\n",
    "                authlink = link.get(\"href\")\n",
    "                author_links.append(authlink)\n",
    "            if get_abstract is None:\n",
    "                abstract = \" \"\n",
    "            else: \n",
    "                abstract = get_abstract.get_text()\n",
    "            publication_details.update([\n",
    "                                (\"publication title\", publication_title),\n",
    "                                (\"abstract\",abstract),\n",
    "                                (\"authors\", authors),\n",
    "                                (\"publication link\", publication_link),\n",
    "                                (\"authors link\", author_links),\n",
    "                                (\"publication date\", date)\n",
    "                                ])\n",
    "            crawled_data.append(publication_details)\n",
    "            with open(\"database.json\", \"w\") as jsonfile:\n",
    "                json.dump(crawled_data, jsonfile, indent=4)\n",
    "    else:\n",
    "        print(f\"Crawling Completed with {count} number of pages\")\n",
    "        \n",
    "mycrawler(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "User-Agent: *\n",
    "Crawl-Delay: 1\n",
    "Disallow: /*?*format=rss\n",
    "Disallow: /*?*export=xls\n",
    "Sitemap: https://pureportal.coventry.ac.uk/sitemap.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "ps = PorterStemmer()\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "\n",
    "def word_processor(tokens):\n",
    "    stem_sentence = []\n",
    "    text = str(tokens).lower() # converts all words to lower case  \n",
    "    text = text.translate(str.maketrans('','', string.punctuation)) # removes punctuations\n",
    "    token_words= word_tokenize(text) #tokennize each word\n",
    "    for word in token_words:\n",
    "        if word not in sw:   \n",
    "            word = ps.stem(word)\n",
    "            stem_sentence.append(word)          \n",
    "            \n",
    "    return stem_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open('database.json')\n",
    "data = json.load(file)\n",
    "\n",
    "\n",
    "def inverted_index(data):\n",
    "    inv_index = {} # stores inverted \n",
    "    for document in data:\n",
    "        for key,value in document.items():\n",
    "            if 'http' in value: #ignoring urls\n",
    "                continue\n",
    "            if type(value) is list: #profile links were saved in a list so we ignorne them \n",
    "                continue\n",
    "            doc_token = word_processor(value)  #applying preprocessing on all words in the data base\n",
    "            doc_index = data.index(document) #creating the index for each token\n",
    "            for word in doc_token:\n",
    "                if word in inv_index:\n",
    "                    inv_index[word].append(doc_index)\n",
    "                else:\n",
    "                    inv_index[word] = [doc_index]\n",
    "    # grouping the DocID of each document by the number of times they occur an\n",
    "    for word in inv_index:\n",
    "        w = (inv_index[word])\n",
    "        inv_index[word] = [(i, w.count(i)) for i in set(w)] \n",
    "    return inv_index\n",
    "\n",
    "inverted_index(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def query_processor(data,query,index):\n",
    "    doc_len = len(data)\n",
    "    processed_query = word_processor(query)\n",
    "    retrieved_id = [] #stores a ranked list of documents that matches query in descending order \n",
    "    score_list = []   # a list of all (docID , tf_idf_score)\n",
    "    score_dict = {}  # a dictionary containing  (docID , [tf_idf_score])\n",
    "    for word in processed_query:       \n",
    "        if word in index.keys():\n",
    "            for i in range(0, len(index[word])):\n",
    "                tf = math.log10(1 + index[word][i][1])\n",
    "                idf = math.log10(1 + doc_len/(len(index[word])))\n",
    "                tf_idf = round((tf*idf), 2)           \n",
    "                score_list.append([index[word][i][0], tf_idf])\n",
    "    for score in score_list:\n",
    "        if score[0] in score_dict:\n",
    "            score_dict[score[0]].append(score[1])\n",
    "        else:\n",
    "            score_dict[score[0]] = []\n",
    "            score_dict[score[0]].append(score[1])               \n",
    "        # taking the sum of all the scores in the list of the score_dict values\n",
    "    weighted_score = {key: round(sum(score_dict[key]),2) for key in score_dict}\n",
    "    sort_dict = dict(sorted(weighted_score.items(), key=lambda item: item[1], reverse=True)) # sorts the score_dict in descending order\n",
    "    for key,value in sort_dict.items():\n",
    "        retrieved_id.append(key)  \n",
    "    print(f\"Your search returned {len(retrieved_id)} result\")\n",
    "    print(\"\"\" \"\"\")\n",
    "    for i in retrieved_id:\n",
    "        print(\"Publication Title: \", data[i]['publication title'])\n",
    "        print(\"Publication link: \", data[i]['publication link'])\n",
    "        print(\"Abstract: \", data[i]['abstract'][:400] +'............')\n",
    "        print(\"Authors: \", data[i]['authors'])\n",
    "        print(\"Publication Date: \", data[i]['publication date'])       \n",
    "        print( ''' ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_query():\n",
    "    file = open('database.json')\n",
    "    data = json.load(file)\n",
    "    query = input(\"Enter query to search: \")\n",
    "    index = inverted_index(data)\n",
    "    retrieved = query_processor(data,query,index)\n",
    "    \n",
    "    return retrieved\n",
    "\n",
    "get_query()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('sci_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9c68b767de50f3d2a028f1fb165139ae3fb5650c2abb9aa62c9fc2c156fa366"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
